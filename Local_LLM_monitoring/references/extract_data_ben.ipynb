{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecc465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from psycopg2.extras import Json\n",
    "from pgvector.psycopg2 import register_vector\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "import hashlib\n",
    "import uuid\n",
    "import time\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a367df34",
   "metadata": {},
   "source": [
    "# UTILITY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9e93b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to directly execute a query\n",
    "def postgres_execute(conn, query, get_result=False, commit=True, print_query=False):\n",
    "    if print_query: \n",
    "        print(query)\n",
    "    with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cursor:\n",
    "    #with conn.cursor() as cursor:\n",
    "        cursor.execute(query)\n",
    "        if commit:\n",
    "            conn.commit()\n",
    "        if get_result:\n",
    "            return cursor.fetchall()\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a0383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced get_ollama_embedding function with model parameter\n",
    "def get_ollama_embedding(text, myModel=\"mistral\"):\n",
    "    response = ollama.embeddings(model=myModel, prompt=text)\n",
    "    embedding = response[\"embedding\"]\n",
    "    token_used = -1 # Ollama doesn't return the token used\n",
    "    result = {}\n",
    "    result[\"embedding\"] = embedding\n",
    "    result[\"token\"] = token_used\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0939736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query similar documents with enhanced model selection\n",
    "def query_documents(embedding_method, question, cur, conn, query_ledger_uuid,n_results=3, myModel=\"mistral\", document_uuid=None,username='system'):\n",
    "    \"\"\"Query documents for relevant chunks based on embedding similarity.\n",
    "    \n",
    "    Args:\n",
    "        embedding_method (str): Method for embeddings ('OLLAMA' or 'OPENAI')\n",
    "        clientopenAI: OpenAI client (required if embedding_method is 'OPENAI')\n",
    "        question (str): The question to search for\n",
    "        cur: Database cursor\n",
    "        conn: Database connection\n",
    "        n_results (int): Number of top chunks to return (default: 3)\n",
    "        myModel (str): Model name for OLLAMA embeddings (default: 'mistral')\n",
    "        document_uuid (str, optional): If provided, search only within this document\n",
    "        \n",
    "    Returns:\n",
    "        list: List of chunks ordered by relevance (vector similarity)\n",
    "    \"\"\"\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # -------------------------- WE EXTRACT THE SUB SET OF CHUNCK TO EMBED -------------\n",
    "    start_time = time.time()\n",
    "    targetChunckTable = \"\"\n",
    "    embedding_method == \"OLLAMA\"\n",
    "    # Using the enhanced table for generic embedding models\n",
    "    targetChunckTable = \"document_embeding_mistral_generic\"\n",
    "    query_embedding = get_ollama_embedding(question, myModel)['embedding']\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Time to get the embedding: \", end_time - start_time)\n",
    "    print(f\"Query: {question}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Base query with all fields\n",
    "    query = \"\"\"SELECT \n",
    "        d.document_uuid as document_uuid,\n",
    "        d.document_name as document_name,\n",
    "        d.document_location as document_location,\n",
    "        d.document_hash as document_hash,\n",
    "        d.document_type as document_type,\n",
    "        d.document_status as document_status,\n",
    "        d.tags as document_tags,\n",
    "        dc.document_chunk_uuid as document_chunk_uuid,\n",
    "        dc.embebed_text as embebed_text,\n",
    "        emb.document_embeding_uuid as document_embeding_uuid,\n",
    "        emb.embeder_type as embeder_type,\n",
    "        emb.embedding_token as embedding_token,\n",
    "        emb.embedding_time as embedding_time,\n",
    "        embedding <-> %s::vector as similarity_score\n",
    "    \"\"\"\n",
    "    params = [query_embedding]\n",
    "    \n",
    "    query += \"\"\"\n",
    "    FROM document_library.\"\"\" + targetChunckTable + \"\"\" emb\n",
    "    INNER JOIN document_library.document_chunks dc ON dc.document_chunk_uuid = emb.document_chunk_uuid \n",
    "    INNER JOIN document_library.documents d ON d.document_uuid = dc.document_uuid\n",
    "    inner join document_library.document_security_groups dsg on dsg.document_uuid = d.document_uuid\n",
    "    inner join document_library.security_groups sg on sg.security_group_uuid  = dsg.security_group_uuid\n",
    "    inner join document_library.user_security_groups usg on usg.security_group_uuid = sg.security_group_uuid\n",
    "    inner join document_library.users u on u.user_uuid = usg.user_uuid and u.sso_unique_id = %s\n",
    "    \"\"\"\n",
    "    params.append(username)    \n",
    "    # Add document filter if UUID is provided\n",
    "    if document_uuid:\n",
    "        query += \"\\nWHERE d.document_uuid = %s\"\n",
    "        params.append(document_uuid)\n",
    "\n",
    "    query += \"\\nORDER BY similarity_score ASC\"\n",
    "    \n",
    "    # Add limit\n",
    "    query += \"\\nLIMIT %s\"\n",
    "    params.append(n_results)\n",
    "    \n",
    "    # Execute query\n",
    "    query = cur.mogrify(query, params).decode('utf-8')\n",
    "    #print(query)  # Debug output\n",
    "    results = postgres_execute(conn, query, commit=True, get_result=True, print_query=True)\n",
    "    end_time = time.time()\n",
    "    print(\"Time to get the chunks from the db: \", end_time - start_time)\n",
    "    print(f\"Retrieved {len(results)} chunks\" + (f\" for document {document_uuid}\" if document_uuid else \"\"))\n",
    "    \n",
    "    # If similarity scores were calculated, print them for debugging\n",
    "    if results and 'similarity_score' in results[0]:\n",
    "        print(\"\\nTop chunks by similarity:\")\n",
    "        for i, chunk in enumerate(results[:min(5, len(results))]):\n",
    "            score = chunk['similarity_score']\n",
    "            text_preview = chunk['embebed_text'][:50] + \"...\" if len(chunk['embebed_text']) > 50 else chunk['embebed_text']\n",
    "            print(f\"{i+1}. Score: {score:.6f} - Document: {chunk['document_name']} - Text: {text_preview}\")\n",
    "            query_ledger_chunkk = f\"\"\"\n",
    "            INSERT INTO document_library.query_answer_chunks\n",
    "            (query_answer_chunk_uuid, query_answer_uuid, query_ledger_uuid, chunk_uuid, creation_date, created_by, updated_date, updated_by, \"comments\")    \n",
    "            VALUES ('{str(uuid.uuid4())}', 'to_be_managed_if_needed', '{query_ledger_uuid}', '{chunk['document_chunk_uuid']}', now(), 'system', now(), 'system', 'chunk used for the answer')\n",
    "            \"\"\"\n",
    "            postgres_execute(conn, query_ledger_chunkk, commit=True)\n",
    "    elif results and document_uuid:\n",
    "        print(\"\\nTop chunks for this document:\")\n",
    "        for i, chunk in enumerate(results[:min(5, len(results))]):\n",
    "            text_preview = chunk['embebed_text'][:50] + \"...\" if len(chunk['embebed_text']) > 50 else chunk['embebed_text']\n",
    "            print(f\"{i+1}. Document: {chunk['document_name']} - Text: {text_preview}\")\n",
    "            # we insert in the query ledger the chunk that was used\n",
    "            # the structure of the table is INSERT INTO document_library.query_answer_chunks\n",
    "            #(query_answer_chunk_uuid, query_answer_uuid, query_ledger_uuid, chunk_uuid, creation_date, created_by, updated_date, updated_by, \"comments\")    \n",
    "            query_ledger_chunkk = f\"\"\"\n",
    "            INSERT INTO document_library.query_answer_chunks\n",
    "            (query_answer_chunk_uuid, query_answer_uuid, query_ledger_uuid, chunk_uuid, creation_date, created_by, updated_date, updated_by, \"comments\")    \n",
    "            VALUES ('{str(uuid.uuid4())}', 'to_be_managed_if_needed', '{query_ledger_uuid}', '{chunk['document_chunk_uuid']}', now(), 'system', now(), 'system', 'chunk used for the answer')\n",
    "            \"\"\"\n",
    "            postgres_execute(conn, query_ledger_chunkk, commit=True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0a9d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate response\n",
    "def generate_response(answer_method, clientopenAI, question, relevant_chunks):\n",
    "    context = \"\\n\\n\".join(relevant_chunks)\n",
    "    prompt = (\n",
    "        \"You are an assistant for question-answering tasks. Use the retrieved context below \"\n",
    "        \"to answer the question. If you don't know, say so in three sentences maximum.\"\n",
    "        \"\\n\\nContext:\\n\" + context + \"\\n\\nQuestion:\\n\" + question\n",
    "    )\n",
    "\n",
    "\n",
    "    answer_method == \"OPENAI\"\n",
    "    response = clientopenAI.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033800fb",
   "metadata": {},
   "source": [
    "# METADATA FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c7bfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve metadata fields from the database\n",
    "def get_metadata_fields(conn,cur,target_metadata_uuid=None):\n",
    "    \"\"\"Retrieve metadata fields and their configurations from the database.\n",
    "    \n",
    "    Args:\n",
    "        conn: Database connection\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of metadata fields with their configurations\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            metadata_uuid,\n",
    "            metadata_name,\n",
    "            metadata_description,\n",
    "            metadata_type\n",
    "        FROM document_library.metadatas\n",
    "    \"\"\"\n",
    "    if target_metadata_uuid:\n",
    "        query += \" WHERE metadata_uuid = %s\"\n",
    "        query = cur.mogrify(query, (target_metadata_uuid,)).decode('utf-8')\n",
    "    with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cursor:\n",
    "        cursor.execute(query)\n",
    "        metadata_records = cursor.fetchall()\n",
    "    \n",
    "    # Convert database records to the format needed for extraction\n",
    "    metadata_fields = {}\n",
    "    for record in metadata_records:\n",
    "        # Create a prompt based on the metadata description or use a default prompt\n",
    "        prompt_template = \"You are an expert document analyzer. Extract the {field_description} from this text. Return ONLY the {output_format} or 'Not found' if it cannot be determined. Text: {{text}}\"\n",
    "        \n",
    "        # Map database metadata_type to the appropriate value_type\n",
    "        value_type_mapping = {\n",
    "            \"STRING\": \"string\",\n",
    "            \"NUMBER\": \"float\",\n",
    "            \"INTEGER\": \"int\",\n",
    "            \"DATE\": \"date\",\n",
    "            \"BOOLEAN\": \"boolean\"\n",
    "        }\n",
    "        \n",
    "        # Determine appropriate output format based on metadata type\n",
    "        output_format = \"value\"\n",
    "        if record[\"metadata_type\"] == \"DATE\":\n",
    "            output_format = \"date in DD/MM/YYYY format\"\n",
    "        elif record[\"metadata_type\"] == \"NUMBER\":\n",
    "            output_format = \"number (no currency symbol)\"\n",
    "        elif record[\"metadata_type\"] == \"INTEGER\":\n",
    "            output_format = \"number as an integer\"\n",
    "        elif record[\"metadata_type\"] == \"BOOLEAN\":\n",
    "            output_format = \"'true' or 'false'\"\n",
    "        \n",
    "        value_type = value_type_mapping.get(record[\"metadata_type\"].upper(), \"string\")\n",
    "        \n",
    "        metadata_fields[record[\"metadata_uuid\"]] = {\n",
    "            \"name\": record[\"metadata_name\"],\n",
    "            \"prompt\": prompt_template.format(\n",
    "                field_description=record[\"metadata_description\"], \n",
    "                output_format=output_format\n",
    "            ),\n",
    "            \"confidence_threshold\": 0.7,  # Default confidence threshold\n",
    "            \"value_type\": value_type\n",
    "        }\n",
    "    \n",
    "    return metadata_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646f3cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata_BEN2(document_uuid, conn, cur,client_llm=None, llm_method=\"OPENAI\", overwrite_existing=False, embedding_method=\"OLLAMA\", model_name=\"mistral\", n_results_per_document=3,target_metadata_uuid=None, username=\"bfoucque\"):\n",
    "\n",
    "    # Get metadata fields from database\n",
    "    metadata_fields = get_metadata_fields(conn,cur,target_metadata_uuid)\n",
    "    \n",
    "    print(f\"Starting metadata extraction process for document: {document_uuid}\")\n",
    "    print(f\"Processing {len(metadata_fields)} metadata fields\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = {}\n",
    "    # Process each metadata field from the database\n",
    "    for metadata_uuid, field_config in metadata_fields.items():            \n",
    "        query_ledger_uuid = str(uuid.uuid4())\n",
    "        # we insert in the query ledger the metadata that was used\n",
    "        # the structure is INSERT INTO document_library.query_ledgers_extended\n",
    "        #(query_ledger_uuid, query_type, query_content, metadata_uuid, user_uuid, query_tags, query_start_document_date, query_end_document_date, query_answer, creation_date, created_by, updated_date, updated_by, \"comments\")\n",
    "        query_ledger = f\"\"\"\n",
    "        INSERT INTO document_library.query_ledgers_extended\n",
    "        (query_ledger_uuid, query_type, query_content, metadata_uuid, user_uuid, query_tags, query_start_document_date, query_end_document_date, query_answer, creation_date, created_by, updated_date, updated_by, \"comments\")\n",
    "        VALUES ('{query_ledger_uuid}', 'metadata_extraction', '{field_config[\"prompt\"].replace(\"'\",\"''\")}', '{metadata_uuid}', NULL, 'metadata_extraction', now(), now(), 'to_be_managed_if_needed', now(), 'system', now(), 'system', 'metadata extraction')\n",
    "        \"\"\"\n",
    "        postgres_execute(conn, query_ledger, commit=True)\n",
    "\n",
    "        print(f\"Processing metadata field: {field_config['name']} (UUID: {metadata_uuid})\")\n",
    "        field_start_time = time.time()\n",
    "\n",
    "        # Get all document chunks - modified to follow the flow in metadata_extraction.md\n",
    "        print(f\"Retrieving chunks for document: {document_uuid}\")\n",
    "        \n",
    "        chunks = query_documents(\n",
    "                embedding_method,\n",
    "                field_config['prompt'],\n",
    "                cur,\n",
    "                conn,\n",
    "                query_ledger_uuid=query_ledger_uuid,\n",
    "                n_results=n_results_per_document,\n",
    "                myModel=model_name,\n",
    "                document_uuid=document_uuid,\n",
    "                username=username\n",
    "            )\n",
    "        \n",
    "        \n",
    "        if not chunks:\n",
    "            print(f\"No chunks found for document UUID: {document_uuid}\")\n",
    "            \n",
    "        # Concatenate all chunks text with section markers\n",
    "        all_text = \"\\n\\n---SECTION---\\n\\n\".join([chunk[\"embebed_text\"] for chunk in chunks])\n",
    "        print(f\"Retrieved {len(chunks)} chunks with total length: {len(all_text)} characters\")\n",
    "        \n",
    "        # Format the prompt with the document text - step 3 in the flow\n",
    "        prompt = field_config[\"prompt\"].format(text=all_text)\n",
    "        \n",
    "        # Use the appropriate LLM to extract metadata - step 4 in the flow\n",
    "        if llm_method == \"OPENAI\":\n",
    "            if not client_llm:\n",
    "                raise ValueError(\"OpenAI client required for OPENAI method\")\n",
    "                \n",
    "            response = client_llm.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",  # Can be adjusted based on needs\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=0.1  # Low temperature for more deterministic results\n",
    "            )\n",
    "            raw_value = response.choices[0].message.content.strip()\n",
    "            confidence = 0.9  \n",
    "            model_used = \"gpt-4o-mini\"\n",
    "            \n",
    "        elif llm_method == \"OLLAMA\":\n",
    "            # Using the specific model name parameter\n",
    "            response = ollama.chat(\n",
    "                model=model_name,\n",
    "                messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "            )\n",
    "            raw_value = response['message']['content'].strip()\n",
    "            confidence = 0.85\n",
    "            model_used = model_name\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported LLM method: {llm_method}\")\n",
    "        \n",
    "        # Process the value based on the expected type\n",
    "        value_type = field_config[\"value_type\"]\n",
    "        processed_value = None\n",
    "        \n",
    "        try:\n",
    "            if value_type == \"float\":\n",
    "                if raw_value.lower() == 'not found' or raw_value == '-1':\n",
    "                    processed_value = None\n",
    "                else:\n",
    "                    numeric_str = ''.join(c for c in raw_value if c.isdigit() or c in '.-')\n",
    "                    processed_value = float(numeric_str) if numeric_str else None\n",
    "            \n",
    "            elif value_type == \"int\":\n",
    "                if raw_value.lower() == 'not found' or raw_value == '-1':\n",
    "                    processed_value = None\n",
    "                else:\n",
    "                    numeric_str = ''.join(c for c in raw_value if c.isdigit() or c == '-')\n",
    "                    processed_value = int(numeric_str) if numeric_str else None\n",
    "            \n",
    "            elif value_type == \"date\":\n",
    "                if raw_value.lower() == 'not found':\n",
    "                    processed_value = None\n",
    "                else:\n",
    "                    import datetime\n",
    "                    from dateutil import parser\n",
    "                    try:\n",
    "                        # First try DD/MM/YYYY format\n",
    "                        if '/' in raw_value:\n",
    "                            day, month, year = map(int, raw_value.split('/'))\n",
    "                            processed_value = datetime.date(year, month, day)\n",
    "                        else:\n",
    "                            processed_value = parser.parse(raw_value).date()\n",
    "                    except (ValueError, parser.ParserError):\n",
    "                        processed_value = None\n",
    "            \n",
    "            elif value_type == \"boolean\":\n",
    "                processed_value = raw_value.lower() == 'true'\n",
    "            \n",
    "            elif value_type == \"string\":\n",
    "                processed_value = raw_value if raw_value.lower() != 'not found' else None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing value for {field_config['name']}: {e}\")\n",
    "            processed_value = None\n",
    "        \n",
    "        # Record the result with both raw and processed values\n",
    "        results[metadata_uuid] = {\n",
    "            \"name\": field_config['name'],\n",
    "            \"raw_value\": raw_value,\n",
    "            \"processed_value\": processed_value,\n",
    "            \"value_type\": value_type,\n",
    "            \"confidence\": confidence,\n",
    "            \"processing_time\": time.time() - field_start_time\n",
    "        }\n",
    "        \n",
    "        # MODIFIED SECTION: Check if metadata already exists for this document and field\n",
    "        check_query = f\"\"\"\n",
    "            SELECT document_metadata_uuid \n",
    "            FROM document_library.document_metadatas \n",
    "            WHERE document_uuid = '{document_uuid}' \n",
    "            AND metadata_uuid = '{metadata_uuid}'\n",
    "        \"\"\"\n",
    "        existing_metadata = postgres_execute(conn, check_query, get_result=True)\n",
    "        \n",
    "        # Build metadata values dictionary with the processed value\n",
    "        metadata_values = {\n",
    "            \"document_uuid\": document_uuid,\n",
    "            \"metadata_uuid\": metadata_uuid,\n",
    "            \"updated_date\": \"now()\",\n",
    "            \"updated_by\": \"SYSTEM\",\n",
    "            \"query_ledger_uuid\": query_ledger_uuid,\n",
    "            \"comments\": f\"Extracted by {llm_method} model: {model_used}. Confidence: {confidence:.2f}\"\n",
    "        }\n",
    "        \n",
    "        # Set the appropriate value column based on type\n",
    "        if value_type == \"float\" and processed_value is not None:\n",
    "            metadata_values[\"metadata_value_float\"] = processed_value\n",
    "        elif value_type == \"string\" and processed_value is not None:\n",
    "            metadata_values[\"metadata_value_string\"] = processed_value\n",
    "        elif value_type == \"int\" and processed_value is not None:\n",
    "            metadata_values[\"metadata_value_int\"] = processed_value\n",
    "        elif value_type == \"date\" and processed_value is not None:\n",
    "            metadata_values[\"metadata_value_date\"] = processed_value\n",
    "        elif value_type == \"boolean\":\n",
    "            metadata_values[\"metadata_value_boolean\"] = processed_value\n",
    "            \n",
    "        try:\n",
    "            if existing_metadata and (overwrite_existing or len(existing_metadata) > 0):\n",
    "                # Update existing record\n",
    "                document_metadata_uuid = existing_metadata[0]['document_metadata_uuid']\n",
    "                set_clauses = []\n",
    "                params = []\n",
    "                \n",
    "                for key, value in metadata_values.items():\n",
    "                    if key not in [\"document_uuid\", \"metadata_uuid\"]:  # Skip primary key fields\n",
    "                        set_clauses.append(f\"{key} = %s\")\n",
    "                        params.append(value)\n",
    "                \n",
    "                # Add WHERE clause\n",
    "                params.append(document_metadata_uuid)\n",
    "                \n",
    "                update_query = f\"\"\"\n",
    "                    UPDATE document_library.document_metadatas \n",
    "                    SET {\", \".join(set_clauses)}\n",
    "                    WHERE document_metadata_uuid = %s\n",
    "                \"\"\"\n",
    "                \n",
    "                with conn.cursor() as cursor:\n",
    "                    cursor.execute(update_query, params)\n",
    "                conn.commit()\n",
    "                print(f\"  - Updated {field_config['name']}: {raw_value[:50]}{'...' if len(raw_value) > 50 else ''}\")\n",
    "                \n",
    "            else:\n",
    "                # Insert new record\n",
    "                document_metadata_uuid = str(uuid.uuid4())\n",
    "                metadata_values[\"document_metadata_uuid\"] = document_metadata_uuid\n",
    "                metadata_values[\"creation_date\"] = \"now()\"\n",
    "                metadata_values[\"created_by\"] = \"SYSTEM\"\n",
    "                \n",
    "                # Create SQL query for inserting metadata\n",
    "                query = sql.SQL(\"INSERT INTO document_library.document_metadatas ({}) VALUES ({})\").format(\n",
    "                    sql.SQL(', ').join(map(sql.Identifier, metadata_values.keys())),\n",
    "                    sql.SQL(', ').join(map(sql.Literal, metadata_values.values()))\n",
    "                )\n",
    "                query = query.as_string(conn)\n",
    "                \n",
    "                with conn.cursor() as cursor:\n",
    "                    cursor.execute(query)\n",
    "                conn.commit()\n",
    "                print(f\"  - Inserted {field_config['name']}: {raw_value[:50]}{'...' if len(raw_value) > 50 else ''}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving metadata: {e}\")\n",
    "        \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Metadata extraction completed in {total_time:.2f} seconds\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1290592",
   "metadata": {},
   "source": [
    "# 2. ENVIRONMENT CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53fafea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI client\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client_OPENAI = OpenAI(api_key=openai_key)\n",
    "\n",
    "# Initialize PostgreSQL connection\n",
    "conn = psycopg2.connect(\n",
    "    dbname=os.getenv(\"DB_NAME\"),\n",
    "    user=os.getenv(\"DB_USER\"),\n",
    "    host=os.getenv(\"DB_HOST\"),\n",
    "    port=os.getenv(\"DB_PORT\"),\n",
    "    password=os.getenv(\"DB_PASSWORD\"),\n",
    ")\n",
    "cur = conn.cursor()\n",
    "conn.autocommit = True\n",
    "\n",
    "# Register the vector extension\n",
    "register_vector(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1002fa08",
   "metadata": {},
   "source": [
    "# DATABASE PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd544a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECTOR EXTENSION\n",
    "query = \"CREATE EXTENSION IF NOT EXISTS vector;\"\n",
    "postgres_execute(conn, query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c682295b",
   "metadata": {},
   "source": [
    "## 8.2 Extract Metadata (OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for the enhanced metadata extraction function with model selection\n",
    "document_uuid_to_process = '26ca4b89-7af4-4527-961f-b8db48ec9e95'  # Replace with actual document UUID\n",
    "#query_ledger_uuid = str(uuid.uuid4())\n",
    "# Extract metadata using OpenAI\n",
    "metadata_results = extract_metadata_BEN2(\n",
    "    document_uuid=document_uuid_to_process,\n",
    "    conn=conn,\n",
    "    cur=cur,\n",
    "    #query_ledger_uuid=query_ledger_uuid,\n",
    "    client_llm=client_OPENAI,\n",
    "    llm_method=\"OPENAI\",\n",
    "    overwrite_existing=True,\n",
    "    model_name=\"nomic-embed-text\",\n",
    "    #target_metadata_uuid='650ffea0-2c50-457b-8b24-ab2aeae4a13a',\n",
    "    username='bfoucque@palo-it.com',\n",
    ")\n",
    "\n",
    "print(\"\\nExtracted Metadata Summary:\")\n",
    "for field_name, field_data in metadata_results.items():\n",
    "    print(f\"{field_name}: {field_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7379a92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, retrieve all document UUIDs from the database\n",
    "query = \"SELECT document_uuid, document_name FROM document_library.documents ORDER BY document_name\"\n",
    "documents = postgres_execute(conn, query, get_result=True)\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    document_uuid_to_process = doc['document_uuid']\n",
    "    #query_ledger_uuid = str(uuid.uuid4())\n",
    "\n",
    "    # Extract metadata using OpenAI\n",
    "    metadata_results = extract_metadata_BEN2(\n",
    "        document_uuid=document_uuid_to_process,\n",
    "        conn=conn,\n",
    "        cur=cur,\n",
    "        #query_ledger_uuid=query_ledger_uuid,\n",
    "        client_llm=client_OPENAI,\n",
    "        llm_method=\"OPENAI\",\n",
    "        overwrite_existing=True,\n",
    "        model_name=\"nomic-embed-text\",\n",
    "        #target_metadata_uuid='650ffea0-2c50-457b-8b24-ab2aeae4a13a',\n",
    "        username='bfoucque@palo-it.com',\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed26247b",
   "metadata": {},
   "source": [
    "## 8.3 Extract Metadata (Ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8b6007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metadata using Ollama with a custom model\n",
    "ollama_metadata_results = extract_metadata_BEN(\n",
    "    document_uuid=document_uuid_to_process,\n",
    "    conn=conn,\n",
    "    llm_method=\"OLLAMA\",\n",
    "    overwrite_existing=False,\n",
    "    model_name=\"nomic-embed-text\"  # Using the enhanced model from pgvector_ben_search\n",
    ")\n",
    "\n",
    "print(\"\\nOllama-Extracted Metadata Summary:\")\n",
    "for field_name, field_data in ollama_metadata_results.items():\n",
    "    print(f\"{field_name}: {field_data}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvassily",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
