{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd8d12a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f4ce56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18()\n",
    "inputs = torch.randn(5, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0113d89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b65f5d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  model_inference         4.14%       3.663ms       100.00%      88.432ms      88.432ms             1  \n",
      "                     aten::conv2d         0.13%     113.752us        59.36%      52.496ms       2.625ms            20  \n",
      "                aten::convolution         0.36%     320.324us        59.23%      52.382ms       2.619ms            20  \n",
      "               aten::_convolution         0.29%     260.744us        58.87%      52.062ms       2.603ms            20  \n",
      "                aten::thnn_conv2d         0.09%      83.789us        58.55%      51.778ms       2.589ms            20  \n",
      "       aten::_slow_conv2d_forward        58.29%      51.545ms        58.46%      51.694ms       2.585ms            20  \n",
      "                 aten::max_pool2d         0.07%      58.498us        18.07%      15.980ms      15.980ms             1  \n",
      "    aten::max_pool2d_with_indices        18.00%      15.922ms        18.00%      15.922ms      15.922ms             1  \n",
      "                 aten::batch_norm         0.09%      78.958us        12.85%      11.361ms     568.045us            20  \n",
      "     aten::_batch_norm_impl_index         0.12%     109.625us        12.76%      11.282ms     564.097us            20  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 88.432ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "075a0a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls                                                                      Input Shapes  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "                  model_inference         4.14%       3.663ms       100.00%      88.432ms      88.432ms             1                                                                                []  \n",
      "                     aten::conv2d         0.03%      22.292us        21.78%      19.261ms       4.815ms             4                             [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], []]  \n",
      "                aten::convolution         0.10%      90.164us        21.75%      19.238ms       4.810ms             4                     [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], [], [], []]  \n",
      "               aten::_convolution         0.07%      59.040us        21.65%      19.148ms       4.787ms             4     [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], [], [], [], [], [], [], []]  \n",
      "                aten::thnn_conv2d         0.02%      20.418us        21.58%      19.087ms       4.772ms             4                                 [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], []]  \n",
      "       aten::_slow_conv2d_forward        21.50%      19.011ms        21.56%      19.067ms       4.767ms             4                                 [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], []]  \n",
      "                 aten::max_pool2d         0.07%      58.498us        18.07%      15.980ms      15.980ms             1                                           [[5, 64, 112, 112], [], [], [], [], []]  \n",
      "    aten::max_pool2d_with_indices        18.00%      15.922ms        18.00%      15.922ms      15.922ms             1                                           [[5, 64, 112, 112], [], [], [], [], []]  \n",
      "                     aten::conv2d         0.06%      51.124us        12.26%      10.844ms      10.844ms             1                             [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], []]  \n",
      "                aten::convolution         0.13%     117.915us        12.20%      10.793ms      10.793ms             1                     [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], [], [], []]  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "Self CPU time total: 88.432ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c117f562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor XPU devices are available to demonstrate profiling on acceleration devices\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vassilylombard/GITHUB_PALO-IT/LAB-VASSILY/pyvassily/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3680: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.xpu.is_available():\n",
    "    device = 'xpu'\n",
    "else:\n",
    "    print('Neither CUDA nor XPU devices are available to demonstrate profiling on acceleration devices')\n",
    "    import sys\n",
    "    sys.exit(0)\n",
    "\n",
    "activities = [ProfilerActivity.CPU, ProfilerActivity.CUDA, ProfilerActivity.XPU]\n",
    "sort_by_keyword = device + \"_time_total\"\n",
    "\n",
    "model = models.resnet18().to(device)\n",
    "inputs = torch.randn(5, 3, 224, 224).to(device)\n",
    "\n",
    "with profile(activities=activities, record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=sort_by_keyword, row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1e7428f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                      aten::empty         0.48%     337.340us         0.48%     337.340us       1.687us     320.22 Mb     320.22 Mb           200  \n",
      "                    aten::resize_         0.09%      60.912us         0.09%      60.912us       3.046us      46.42 Mb      46.42 Mb            20  \n",
      "    aten::max_pool2d_with_indices         6.14%       4.347ms         6.14%       4.347ms       4.347ms      11.48 Mb      11.48 Mb             1  \n",
      "                 aten::empty_like         0.03%      24.001us         0.08%      58.999us       2.950us      47.37 Mb     980.00 Kb            20  \n",
      "                      aten::addmm         0.51%     360.460us         0.52%     366.543us     366.543us      19.53 Kb      19.53 Kb             1  \n",
      "                       aten::mean         0.03%      22.749us         0.18%     126.458us     126.458us      10.00 Kb       9.99 Kb             1  \n",
      "                       aten::div_         0.03%      19.417us         0.09%      62.834us      62.834us           8 b           4 b             1  \n",
      "              aten::empty_strided         0.00%       1.958us         0.00%       1.958us       1.958us           4 b           4 b             1  \n",
      "                     aten::conv2d         0.09%      60.753us        70.48%      49.897ms       2.495ms      47.37 Mb           0 b            20  \n",
      "                aten::convolution         0.35%     249.495us        70.40%      49.836ms       2.492ms      47.37 Mb           0 b            20  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 70.793ms\n",
      "\n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                      aten::empty         0.48%     337.340us         0.48%     337.340us       1.687us     320.22 Mb     320.22 Mb           200  \n",
      "                 aten::batch_norm         0.07%      48.547us        16.03%      11.351ms     567.536us      47.41 Mb           0 b            20  \n",
      "     aten::_batch_norm_impl_index         0.13%      95.411us        15.97%      11.302ms     565.109us      47.41 Mb           0 b            20  \n",
      "          aten::native_batch_norm        15.56%      11.017ms        15.81%      11.193ms     559.670us      47.41 Mb     -56.50 Kb            20  \n",
      "                     aten::conv2d         0.09%      60.753us        70.48%      49.897ms       2.495ms      47.37 Mb           0 b            20  \n",
      "                aten::convolution         0.35%     249.495us        70.40%      49.836ms       2.492ms      47.37 Mb           0 b            20  \n",
      "               aten::_convolution         0.23%     165.253us        70.04%      49.587ms       2.479ms      47.37 Mb           0 b            20  \n",
      "                aten::thnn_conv2d         0.08%      58.710us        69.80%      49.415ms       2.471ms      47.37 Mb           0 b            20  \n",
      "       aten::_slow_conv2d_forward        69.34%      49.090ms        69.72%      49.356ms       2.468ms      47.37 Mb    -272.75 Mb            20  \n",
      "                 aten::empty_like         0.03%      24.001us         0.08%      58.999us       2.950us      47.37 Mb     980.00 Kb            20  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 70.793ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18()\n",
    "inputs = torch.randn(5, 3, 224, 224)\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU],\n",
    "        profile_memory=True, record_shapes=True) as prof:\n",
    "    model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))\n",
    "\n",
    "# (omitting some columns)\n",
    "# ---------------------------------  ------------  ------------  ------------\n",
    "#                              Name       CPU Mem  Self CPU Mem    # of Calls\n",
    "# ---------------------------------  ------------  ------------  ------------\n",
    "#                       aten::empty      94.79 Mb      94.79 Mb           121\n",
    "#     aten::max_pool2d_with_indices      11.48 Mb      11.48 Mb             1\n",
    "#                       aten::addmm      19.53 Kb      19.53 Kb             1\n",
    "#               aten::empty_strided         572 b         572 b            25\n",
    "#                     aten::resize_         240 b         240 b             6\n",
    "#                         aten::abs         480 b         240 b             4\n",
    "#                         aten::add         160 b         160 b            20\n",
    "#               aten::masked_select         120 b         112 b             1\n",
    "#                          aten::ne         122 b          53 b             6\n",
    "#                          aten::eq          60 b          30 b             2\n",
    "# ---------------------------------  ------------  ------------  ------------\n",
    "# Self CPU time total: 53.064ms\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b37817d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m device = \u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      3\u001b[39m activities = [ProfilerActivity.CPU, ProfilerActivity.CUDA, ProfilerActivity.XPU]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model = \u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresnet18\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m inputs = torch.randn(\u001b[32m5\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m224\u001b[39m, \u001b[32m224\u001b[39m).to(device)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m profile(activities=activities) \u001b[38;5;28;01mas\u001b[39;00m prof:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB_PALO-IT/LAB-VASSILY/pyvassily/lib/python3.13/site-packages/torch/nn/modules/module.py:1355\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1352\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1353\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB_PALO-IT/LAB-VASSILY/pyvassily/lib/python3.13/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB_PALO-IT/LAB-VASSILY/pyvassily/lib/python3.13/site-packages/torch/nn/modules/module.py:942\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    943\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    945\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB_PALO-IT/LAB-VASSILY/pyvassily/lib/python3.13/site-packages/torch/nn/modules/module.py:1341\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1334\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1336\u001b[39m             device,\n\u001b[32m   1337\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1338\u001b[39m             non_blocking,\n\u001b[32m   1339\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1340\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB_PALO-IT/LAB-VASSILY/pyvassily/lib/python3.13/site-packages/torch/cuda/__init__.py:363\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    359\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    360\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultiprocessing, you must use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m start method\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    361\u001b[39m     )\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch._C, \u001b[33m\"\u001b[39m\u001b[33m_cuda_getDeviceCount\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTorch not compiled with CUDA enabled\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    366\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    367\u001b[39m     )\n",
      "\u001b[31mAssertionError\u001b[39m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "activities = [ProfilerActivity.CPU, ProfilerActivity.CUDA, ProfilerActivity.XPU]\n",
    "\n",
    "model = models.resnet18().to(device)\n",
    "inputs = torch.randn(5, 3, 224, 224).to(device)\n",
    "\n",
    "with profile(activities=activities) as prof:\n",
    "    model(inputs)\n",
    "\n",
    "prof.export_chrome_trace(\"trace.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b872abf",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sort_by_keyword = \"self_\" + device + \"_time_total\"\n",
    "\n",
    "with profile(\n",
    "    activities=activities,\n",
    "    with_stack=True,\n",
    ") as prof:\n",
    "    model(inputs)\n",
    "\n",
    "# Print aggregated stats\n",
    "print(prof.key_averages(group_by_stack_n=5).table(sort_by=sort_by_keyword, row_limit=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab37601",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m sort_by_keyword = \u001b[33m\"\u001b[39m\u001b[33mself_\u001b[39m\u001b[33m\"\u001b[39m + \u001b[43mdevice\u001b[49m + \u001b[33m\"\u001b[39m\u001b[33m_time_total\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrace_handler\u001b[39m(p):\n\u001b[32m      4\u001b[39m     output = p.key_averages().table(sort_by=sort_by_keyword, row_limit=\u001b[32m10\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "sort_by_keyword = \"self_\" + device + \"_time_total\"\n",
    "\n",
    "def trace_handler(p):\n",
    "    output = p.key_averages().table(sort_by=sort_by_keyword, row_limit=10)\n",
    "    print(output)\n",
    "    p.export_chrome_trace(\"/tmp/trace_\" + str(p.step_num) + \".json\")\n",
    "\n",
    "with profile(\n",
    "    activities=activities,\n",
    "    schedule=torch.profiler.schedule(\n",
    "        wait=1,\n",
    "        warmup=1,\n",
    "        active=2),\n",
    "    on_trace_ready=trace_handler\n",
    ") as p:\n",
    "    for idx in range(8):\n",
    "        model(inputs)\n",
    "        p.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2051ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvassily",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
